# -*- coding: utf-8 -*-
"""Patient Outcome Prediction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a8IoAhxgE-0qSJDDAF7_xr46xq7uJpDT

# Load Dataset
"""

!pip install datasets

from datasets import load_dataset

# Load Data
ds = load_dataset("BI55/MedText")
df = ds['train'].to_pandas()

# Explore Data Structure
print(ds)
print(df.head(10))

"""# Data Cleaning

"""

!pip install textacy==0.12.0
!pip install spacy
!python -m spacy download en_core_web_sm
!pip install contractions
!pip install unidecode

import textacy
from textacy import preprocessing
import re
import spacy
import contractions
import unicodedata
import pandas as pd

nlp = spacy.load('en_core_web_sm')

class CleanData:
    def __init__(self, dataframe, columns):
        self.dataframe = dataframe
        self.columns = columns

    def normalize(self):
        for column in self.columns:
            self.dataframe[column] = self.dataframe[column].apply(lambda x: re.sub(r'\s+', ' ', x).strip() if isinstance(x, str) else x)
        return self

    def convert_to_lowercase(self):
        for column in self.columns:
            self.dataframe[column] = self.dataframe[column].apply(lambda x: x.lower() if isinstance(x, str) else x)
        return self

    def remove_stopwords(self, stopwords):
        for column in self.columns:
            self.dataframe[column] = self.dataframe[column].apply(
                lambda x: ' '.join([word for word in x.split() if word not in stopwords]) if isinstance(x, str) else x
            )
        return self

    def remove_special_characters(self):
        for column in self.columns:
            self.dataframe[column] = self.dataframe[column].apply(lambda x: re.sub(r'[^a-zA-Z0-9\s]', '', x) if isinstance(x, str) else x)
        return self

    def lemmatize(self):
        for column in self.columns:
            self.dataframe[column] = self.dataframe[column].apply(lambda x: ' '.join([token.lemma_ for token in nlp(x)]))
        return self

    def expand_contractions(self):
        for column in self.columns:
            self.dataframe[column] = self.dataframe[column].apply(lambda x: contractions.fix(x))
        return self

    def remove_html_and_urls(self):
        for column in self.columns:
            self.dataframe[column] = self.dataframe[column].apply(lambda x: re.sub(r'http\S+|www.\S+', '', x))
        return self

    def remove_accents(self):
        for column in self.columns:
            self.dataframe[column] = self.dataframe[column].apply(lambda x: ''.join(c for c in unicodedata.normalize('NFD', x) if unicodedata.category(c) != 'Mn'))
        return self


columns = ['Prompt', 'Completion']
cleaner = CleanData(df, columns)
stopwords = ['is', 'a', 'the', 'and']
cleaner.normalize().convert_to_lowercase().remove_stopwords(stopwords).remove_special_characters().lemmatize().expand_contractions().remove_html_and_urls().remove_accents()

print(df.head(10))

"""# Exploratory Data Analysis"""

# Inspection and Overview of data structure
print(df.head(10))
print(df.describe())
print(df.info())
print(df.isnull().sum())

# Document Term Matrix
from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer(analyzer='word')
dtm = vectorizer.fit_transform(df['Completion'])
dtm_df = pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names_out())
print(dtm_df.head(10))

# Tokenization
df['Prompt_tokens'] = df['Prompt'].apply(lambda x: len(x.split()))
df['Completion_tokens'] = df['Completion'].apply(lambda x: len(x.split()))

# Word count per entry
df['Prompt_word_count'] = df['Prompt'].apply(lambda x: len(x.split()))
df['Completion_word_count'] = df['Completion'].apply(lambda x: len(x.split()))

# Visualize word count distribution
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
sns.histplot(df['Prompt_word_count'], bins=30, color='blue', label='Prompt')
sns.histplot(df['Completion_word_count'], bins=30, color='green', label='Completion')
plt.legend()
plt.title('Word Count Distribution')
plt.xlabel('Word Count')
plt.show()

# Word Frequency Analysis
from collections import Counter

all_prompt_words = ' '.join(df['Prompt']).split()
all_completion_words = ' '.join(df['Completion']).split()

prompt_word_counts = Counter(all_prompt_words)
completion_word_counts = Counter(all_completion_words)

print("Top 10 Prompt Words:\n", prompt_word_counts.most_common(10))
print("Top 10 Completion Words:\n", completion_word_counts.most_common(10))

# Text length analysis
df['Prompt_length'] = df['Prompt'].apply(len)
df['Completion_length'] = df['Completion'].apply(len)

# Summary statistics for text length
print("Prompt Length Stats:\n", df['Prompt_length'].describe())
print("Completion Length Stats:\n", df['Completion_length'].describe())

# Frequency Analysis
for column in df.columns:
    print(f"Frequency Analysis for {column}:\n", df[column].value_counts())

# Analyse Bigrams and Trigrams
from nltk import ngrams

all_prompt_bigrams = list(ngrams(all_prompt_words, 2))
all_completion_bigrams = list(ngrams(all_completion_words, 2))

prompt_bigram_counts = Counter(all_prompt_bigrams)
completion_bigram_counts = Counter(all_completion_bigrams)

print("Top 10 Prompt Bigrams:\n", prompt_bigram_counts.most_common(10))
print("Top 10 Completion Bigrams:\n", completion_bigram_counts.most_common(10))

# Histogram of prompt length and completion length
from matplotlib import pyplot as plt

plt.hist(df['Prompt_length'], bins=20, alpha=0.5, label='Prompt')
plt.hist(df['Completion_length'], bins=20, alpha=0.5, label='Completion')
plt.xlabel('Length')
plt.ylabel('Frequency')
plt.title('Prompt and Completion Length Distribution')
plt.legend()
plt.show()